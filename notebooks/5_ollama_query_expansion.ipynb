{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query Optimization](../images/query_optimization.png)\n",
    "\n",
    "In this section, we're going to add one more step (purple) to our RAG pipeline. This is one of my favorite RAG techniques, where we re-write the user's query in different ways, and generate sample answers, before the retrieval step. This is helpful to augment our semantic search with different terms that the user may have ommitted.\n",
    "\n",
    "We can then retrieve documents across these different queries, before aggregating these documents with a custom State Reducer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first copy over our code from our corrective RAG app! I've condensed the functions here for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Fetch retriever\n",
    "retriever = get_vector_db_retriever(\"llama3.2\")\n",
    "\n",
    "# Set up LLM\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# Define our GraphState, InputState, and OutputState\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        attempted_generations: the number of attempted generations\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "    attempted_generations: int\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the input state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "    \"\"\"\n",
    "    question: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the final outputstate of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes and Conditional Edges\n",
    "def retrieve_documents(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    attempted_generations = state.get(\"attempted_generations\", 0)   # By default we set attempted_generations to 0 if it doesn't exist yet\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # RAG generation\n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"attempted_generations\": attempted_generations + 1   # In our state update, we increment attempted_generations\n",
    "    }\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "grade_documents_llm = llm.with_structured_output(GradeDocuments)\n",
    "grade_documents_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_documents_prompt = \"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}\"\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or to terminate execution.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, END---\"\n",
    "        )\n",
    "        return \"none relevant\"    # same as END\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"some relevant\"\n",
    "    \n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "grade_hallucinations_llm = llm.with_structured_output(GradeHallucinations)\n",
    "grade_hallucinations_system_prompt = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "grade_hallucinations_prompt = \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"\n",
    "ATTEMPTED_GENERATION_MAX = 3\n",
    "def grade_hallucinations(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    attempted_generations = state[\"attempted_generations\"]\n",
    "\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    grade_hallucinations_prompt_formatted = grade_hallucinations_prompt.format(\n",
    "        documents=formatted_docs,\n",
    "        generation=generation\n",
    "    )\n",
    "\n",
    "    score = grade_hallucinations_llm.invoke(\n",
    "        [SystemMessage(content=grade_hallucinations_system_prompt)] + [HumanMessage(content=grade_hallucinations_prompt_formatted)]\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        return \"supported\"\n",
    "    elif attempted_generations >= ATTEMPTED_GENERATION_MAX:    # New condition!\n",
    "        print(\"---DECISION: TOO MANY ATTEMPTS, GIVE UP---\")\n",
    "        raise RuntimeError(\"Too many attempted generations with hallucinations, giving up.\")\n",
    "        # return \"give up\"    # Note: We could also do this to silently fail\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind query expansion is that we want to augment the user's original query such that we can retrieve the most helpful documents possible from our document database. There are many techniques for this! Here are two that we'll employ today:\n",
    "1. Re-writing the user's query a number of different ways and using each as a query to the vector DB\n",
    "2. Leaning on the LLM's pre-training to generate some sample answers, and then using these sample answers to query the vector DB. (This is a favorite of mine, oftentimes, pre-training is good enough to approximate what an answer might look like, and that helps with semantic search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RewrittenQueries(BaseModel):\n",
    "    \"\"\"Rewritten queries based on the user's original question.\"\"\"\n",
    "    rewritten_queries: List[str] = Field(\n",
    "        description=\"A list of rewritten versions of the user's query. Each rewritten version is rewritten differently, rephrased and potentially uses synonyms.\"\n",
    "    )\n",
    "\n",
    "rewritten_query_llm = llm.with_structured_output(RewrittenQueries)\n",
    "rewritten_query_system_prompt = \"\"\"You are an analyst in charge of taking a user's question as input, and reframing and rewriting it in different ways.\\n\n",
    "Your goal is to change the phrasing of the question, while making sure that the intent and meaning of the question is the same.\\n\n",
    "Return a list of rewritten_queries. The number will be specified by the user.\"\"\"\n",
    "rewritten_query_prompt = \"Here is the user's question: \\n\\n {question}. Return {num_rewrites} queries.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our prompts on a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "num_rewrites = 3\n",
    "rewritten_query_prompt_formatted = rewritten_query_prompt.format(question=question, num_rewrites=num_rewrites)\n",
    "rewritten_query_llm.invoke(\n",
    "    [SystemMessage(content=rewritten_query_system_prompt)] + [HumanMessage(content=rewritten_query_prompt_formatted)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a prompt and structured output to generate a sample answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SampleAnswer(BaseModel):\n",
    "    \"\"\"Sample answer for an input question.\"\"\"\n",
    "    sample_answer: str = Field(\n",
    "        description=\"A concise example answer for a question. This shouldn't exceed three sentences in length.\"\n",
    "    )\n",
    "\n",
    "sample_answer_llm = llm.with_structured_output(SampleAnswer)\n",
    "sample_answer_system_prompt = \"\"\"You are a novice in charge of taking a user's question as input, and generating a sample answer for it.\\n\n",
    "This sample answer should contain words that would likely be in a real answer, but is not grounded in any factual documents, the way a real answer would be.\"\"\"\n",
    "sample_answer_prompt = \"Here is the user's question: \\n\\n {question}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our sample answer generator too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is it possible to utilize LangGraph for customer support? I'm looking to develop an agent application.\"\n",
    "sample_answer_prompt_formatted = sample_answer_prompt.format(question=question)\n",
    "sample_answer_llm.invoke(\n",
    "    [SystemMessage(content=sample_answer_system_prompt)] + [HumanMessage(content=sample_answer_prompt_formatted)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider the scenario where we want to re-write the query and generate sample responses 3 times. This could be more effective in increasing the diversity of relevant documents we retrieve. \n",
    "\n",
    "However, running these four (3 new + original) sample answer generation + retrieval requests would increase the latency of this step in our pipeline by 4x as well.\n",
    "\n",
    "At that point the latency of this step would outweigh the benefits. How can we circumvent this? By executing these sample answer generation steps in parallel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn about parallelization, let's look at a toy example unrelated to our RAG application.\n",
    "\n",
    "We can define a simple graph with parallel node execution like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "class State(TypedDict):\n",
    "    state: str\n",
    "\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['state']}\")\n",
    "        return {\"state\": [self._value]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, nodes `b` and `c` would execute in parallel.\n",
    "\n",
    "There is an issue however. Both of these nodes want to overwrite the same state key at the same time. Nodes `b` and `c` execute in the same super-step, and thus we have a conflicting update to node `d`.\n",
    "\n",
    "How do we solve this? With a custom state reducer!\n",
    "\n",
    "We add a custom state reducer to the `state` string field using `Annnotated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "class State(TypedDict):\n",
    "    state: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can handle multiple nodes flowing to a single aggregation node with multiple different state updates. We'll see this principle again in our RAG application.\n",
    "\n",
    "One more note on parallelization: what happens in the case where one parallel path has more steps than the other one? \n",
    "\n",
    "LangGraph automatically handles this and waits for all of the parallel nodes in the longer path to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"b2\")\n",
    "builder.add_edge([\"b2\", \"c\"], \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, `b`, `b2`, and `c` are all considered to be part of the same step. All of these will be completed before the graph progresses to node `d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Query Expansion in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand parallel execution, let's add a node to generate three rewritten queries! We will also update our state to keep track of our rewritten queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    rewritten_queries: List[str]    # A new field to track our rewritten queries!\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "    attempted_generations: int\n",
    "\n",
    "\n",
    "def generate_rewritten_queries(state):\n",
    "    \"\"\"\n",
    "    Generates rewritten versions of the original user query\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates rewritten_queries key with a list of rewritten queries\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING REWRITTEN VERSIONS OF THE USER'S QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    num_rewrites = 3\n",
    "    rewritten_query_prompt_formatted = rewritten_query_prompt.format(question=question, num_rewrites=num_rewrites)\n",
    "    response = rewritten_query_llm.invoke(\n",
    "        [SystemMessage(content=rewritten_query_system_prompt)] + [HumanMessage(content=rewritten_query_prompt_formatted)]\n",
    "    )\n",
    "    rewritten_queries = response.rewritten_queries\n",
    "\n",
    "    return {\"rewritten_queries\": rewritten_queries}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add our sample answer generation logic to our `retrieve_documents` node to further improve our semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing a question\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    sample_answer_prompt_formatted = sample_answer_prompt.format(question=question)\n",
    "    response = sample_answer_llm.invoke(\n",
    "        [SystemMessage(content=sample_answer_system_prompt)] + [HumanMessage(content=sample_answer_prompt_formatted)]\n",
    "    )\n",
    "    sample_answer = response.sample_answer\n",
    "    documents = retriever.invoke(f\"{question}: {sample_answer}\")    # Now we use our question and sample answer\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now we need to define edges from our new `generate_rewritten_queries` node to our `retrieve_documents` node. \n",
    "\n",
    "In contrast to earlier, we now need to create multiple edges (one for each rewritten query, and one for the original), and run each of these in parallel.\n",
    "\n",
    "Here is the magic: we use the [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) function to dynamically create edges to the `retrieve_documents` node for each question that is generated, along with our original user's question.\n",
    "\n",
    "This is very useful! It can automatically parallelize document retrieval (and sample answer generation) for any number of rewritten queries.\n",
    "\n",
    "`Send` also allows you to pass any state that you want to `retrieve_documents`! It does not have to align with `OverallState`. In this case, we pass a different question to each `retrieve_documents` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "def continue_to_retrieval_nodes(state: GraphState):\n",
    "    edges_to_create = []\n",
    "    # Add original question\n",
    "    edges_to_create.append(Send(\"retrieve_documents\", {\"question\": state[\"question\"]}))\n",
    "    # Add rewritten queries\n",
    "    for rewritten_query in state[\"rewritten_queries\"]:\n",
    "        edges_to_create.append(Send(\"retrieve_documents\", {\"question\": rewritten_query}))\n",
    "    return edges_to_create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have multiple edges from the `retrieve_documents` node all updating the documents state, we need to make sure that we add a custom state reducer to handle the update. Our functionality here needs to be pretty specific.\n",
    "\n",
    "- Case 1: When the `retrieve_documents` nodes update the documents state, we will simply want to add all of the documents to the list. \n",
    "- Case 2: However, when the `grade_documents` node later filters down to documents that pass our LLM-as-judge relevance test, we still will want to perform a total overwrite. \n",
    "\n",
    "Separately, we will also want to add some logic to the `grade_documents` node that filters down all of our retrieved documents to the most frequently retrieved documents, and only bother grading those (in order to save us some work).\n",
    "\n",
    "Let's write a custom reducer, and then update our logic in the `grade_documents` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_documents_reducer(existing, update):\n",
    "    # If we passed in a dictionary that asks for \"overwrite\", then we return the updated documents only\n",
    "    if isinstance(update, dict) and update[\"type\"] == \"overwrite\":\n",
    "        return update[\"documents\"]\n",
    "\n",
    "    # Otherwise, we simple add the lists\n",
    "    return existing + update\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    rewritten_queries: List[str]\n",
    "    generation: str\n",
    "    documents: Annotated[List[Document], custom_documents_reducer]   # We use Annotated to add our custom reducer\n",
    "    attempted_generations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # -- New logic to deduplicate documents our queries --\n",
    "    doc_counter = Counter(doc.page_content for doc in documents)\n",
    "    most_common_contents = doc_counter.most_common(5)\n",
    "    top_documents = []\n",
    "    for content, _ in most_common_contents:\n",
    "        for d in documents:\n",
    "            if d.page_content == content:\n",
    "                top_documents.append(d)\n",
    "                break\n",
    "\n",
    "    # Score each one of our five most common documents\n",
    "    filtered_docs = []\n",
    "    for d in top_documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": {\"type\": \"overwrite\", \"documents\": filtered_docs}, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we just added a lot of features to our graph. Let's review them as we build our graph!\n",
    "\n",
    "1. We added a new `generate_rewritten_queries` node that generates three rewritten versions of the user's original query\n",
    "2. This node makes use of `Send` to dynamically create multiple edges to our `retrieve_documents` node\n",
    "3. Our `retrieve_documents` node now generates a `sample_answer` for the passed in query, to improve the power of our semantic search\n",
    "4. We wrote a custom reducer for `documents`, so that when our `retrieve_documents` node outputs new documents, they are added to a master list along with the documents from all of the other queries. However, when our `grade_documents` node outputs a filtered down relevant list, we still overwrite our graph state.\n",
    "5. Our `grade_documents` node deduplicates common documents passed in from our `retrieve_documents` nodes, and we only grade the top 5 documents for relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(GraphState, input=InputState, output=OutputState)\n",
    "graph_builder.add_node(\"generate_rewritten_queries\", generate_rewritten_queries)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "\n",
    "graph_builder.add_edge(START, \"generate_rewritten_queries\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_rewritten_queries\",\n",
    "    continue_to_retrieval_nodes,\n",
    "    [\"retrieve_documents\"]\n",
    ")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_response\",\n",
    "    grade_hallucinations,\n",
    "    {\n",
    "        \"supported\": END,\n",
    "        \"not supported\": \"generate_response\"\n",
    "    })\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's watch how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "graph.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We see we generate rewritten version of the query. Then we retrieve documents for each of those queries, and then we grade the top 5 most documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's take a look in LangGraph Studio__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
