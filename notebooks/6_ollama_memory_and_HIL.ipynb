{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Memory, and Human-in-the-Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HIL](../images/hil.png)\n",
    "\n",
    "In this section, we'll cover a series of topics that are important to leveraging the full power of LangGraph. Here are the topics\n",
    "\n",
    "1. Streaming: How does streaming work with LangGraph?\n",
    "2. Memory: How can I persist information in our app across invocations?\n",
    "    - Local memory\n",
    "    - Threads\n",
    "    - External memory\n",
    "3. Breakpoints and Human-in-the-loop\n",
    "\n",
    "After this section, we'll take a closer look at memory and HIL in LangGraph Studio, as well as production monitoring in LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is getting pretty big and complex! Let's copy over what we've done through the first three modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"llama-3.2\" not found, try pulling it first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m load_dotenv(dotenv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../.env\u001b[39m\u001b[38;5;124m\"\u001b[39m, override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Fetch retriever\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mget_vector_db_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Set up LLM\u001b[39;00m\n\u001b[1;32m     22\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOllama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/code/mywork/trainings/langgraph-101/notebooks/utils.py:64\u001b[0m, in \u001b[0;36mget_vector_db_retriever\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m doc_splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(docs_list)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Add to vectorstore\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrag-chroma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(lambda_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retriever\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langgraph-101/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langgraph-101/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    842\u001b[0m     ):\n\u001b[0;32m--> 843\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langgraph-101/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langgraph-101/lib/python3.10/site-packages/langchain_ollama/embeddings.py:161\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed search docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     embedded_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded_docs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langgraph-101/lib/python3.10/site-packages/ollama/_client.py:355\u001b[0m, in \u001b[0;36mClient.embed\u001b[0;34m(self, model, input, truncate, options, keep_alive)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    349\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EmbedResponse:\n\u001b[0;32m--> 355\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/embed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langgraph-101/lib/python3.10/site-packages/ollama/_client.py:175\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langgraph-101/lib/python3.10/site-packages/ollama/_client.py:120\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m   r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 120\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mResponseError\u001b[0m: model \"llama-3.2\" not found, try pulling it first"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT, RAG_PROMPT_WITH_MESSAGES\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.constants import Send\n",
    "from collections import Counter\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "\n",
    "# Fetch retriever\n",
    "retriever = get_vector_db_retriever(\"llama3.2\")\n",
    "\n",
    "# Set up LLM\n",
    "llm = ChatOllama(model=\"llama-3.2\", temperature=0)\n",
    "\n",
    "# Define our GraphState, InputState, and OutputState\n",
    "def custom_documents_reducer(existing, update):\n",
    "    # If we passed in a dictionary that asks for \"overwrite\", then we return the updated documents only\n",
    "    if isinstance(update, dict) and update[\"type\"] == \"overwrite\":\n",
    "        return update[\"documents\"]\n",
    "\n",
    "    # Otherwise, we simple add the lists\n",
    "    return existing + update\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    rewritten_queries: List[str]\n",
    "    generation: str\n",
    "    documents: Annotated[List[Document], custom_documents_reducer]   # We use Annotated to add our custom reducer\n",
    "    attempted_generations: int\n",
    "class InputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the input state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "    \"\"\"\n",
    "    question: str\n",
    "class OutputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the final outputstate of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes and Conditional Edges\n",
    "def retrieve_documents(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing a question\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    sample_answer_prompt_formatted = sample_answer_prompt.format(question=question)\n",
    "    response = sample_answer_llm.invoke(\n",
    "        [SystemMessage(content=sample_answer_system_prompt)] + [HumanMessage(content=sample_answer_prompt_formatted)]\n",
    "    )\n",
    "    sample_answer = response.sample_answer\n",
    "    documents = retriever.invoke(f\"{question}: {sample_answer}\")    # Now we use our question and sample answer\n",
    "    return {\"documents\": documents}\n",
    "def generate_response(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    attempted_generations = state.get(\"attempted_generations\", 0)   # By default we set attempted_generations to 0 if it doesn't exist yet\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # RAG generation\n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"attempted_generations\": attempted_generations + 1   # In our state update, we increment attempted_generations\n",
    "    }\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "grade_documents_llm = llm.with_structured_output(GradeDocuments)\n",
    "grade_documents_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_documents_prompt = \"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}\"\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # -- New logic to deduplicate documents our queries --\n",
    "    doc_counter = Counter(doc.page_content for doc in documents)\n",
    "    most_common_contents = doc_counter.most_common(5)\n",
    "    top_documents = []\n",
    "    for content, _ in most_common_contents:\n",
    "        for d in documents:\n",
    "            if d.page_content == content:\n",
    "                top_documents.append(d)\n",
    "                break\n",
    "\n",
    "    # Score each one of our five most common documents\n",
    "    filtered_docs = []\n",
    "    for d in top_documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": {\"type\": \"overwrite\", \"documents\": filtered_docs}, \"question\": question}\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or to terminate execution.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, END---\"\n",
    "        )\n",
    "        return \"none relevant\"    # same as END\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"some relevant\" \n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "grade_hallucinations_llm = llm.with_structured_output(GradeHallucinations)\n",
    "grade_hallucinations_system_prompt = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "grade_hallucinations_prompt = \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"\n",
    "ATTEMPTED_GENERATION_MAX = 3\n",
    "def grade_hallucinations(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    attempted_generations = state[\"attempted_generations\"]\n",
    "\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    grade_hallucinations_prompt_formatted = grade_hallucinations_prompt.format(\n",
    "        documents=formatted_docs,\n",
    "        generation=generation\n",
    "    )\n",
    "\n",
    "    score = grade_hallucinations_llm.invoke(\n",
    "        [SystemMessage(content=grade_hallucinations_system_prompt)] + [HumanMessage(content=grade_hallucinations_prompt_formatted)]\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        return \"supported\"\n",
    "    elif attempted_generations >= ATTEMPTED_GENERATION_MAX:    # New condition!\n",
    "        print(\"---DECISION: TOO MANY ATTEMPTS, GIVE UP---\")\n",
    "        raise RuntimeError(\"Too many attempted generations with hallucinations, giving up.\")\n",
    "        # return \"give up\"    # Note: We could also do this to silently fail\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "class RewrittenQueries(BaseModel):\n",
    "    \"\"\"Rewritten queries based on the user's original question.\"\"\"\n",
    "    rewritten_queries: List[str] = Field(\n",
    "        description=\"A list of rewritten versions of the user's query. Each rewritten version is rewritten differently, rephrased and potentially uses synonyms.\"\n",
    "    )\n",
    "rewritten_query_llm = llm.with_structured_output(RewrittenQueries)\n",
    "rewritten_query_system_prompt = \"\"\"You are an analyst in charge of taking a user's question as input, and reframing and rewriting it in different ways.\\n\n",
    "Your goal is to change the phrasing of the question, while making sure that the intent and meaning of the question is the same.\\n\n",
    "Return a list of rewritten_queries. The number will be specified by the user.\"\"\"\n",
    "rewritten_query_prompt = \"Here is the user's question: \\n\\n {question}. Return {num_rewrites} queries.\"\n",
    "class SampleAnswer(BaseModel):\n",
    "    \"\"\"Sample answer for an input question.\"\"\"\n",
    "    sample_answer: str = Field(\n",
    "        description=\"A concise example answer for a question. This shouldn't exceed three sentences in length.\"\n",
    "    )\n",
    "sample_answer_llm = llm.with_structured_output(SampleAnswer)\n",
    "sample_answer_system_prompt = \"\"\"You are a novice in charge of taking a user's question as input, and generating a sample answer for it.\\n\n",
    "This sample answer should contain words that would likely be in a real answer, but is not grounded in any factual documents, the way a real answer would be.\"\"\"\n",
    "sample_answer_prompt = \"Here is the user's question: \\n\\n {question}.\"\n",
    "def generate_rewritten_queries(state):\n",
    "    \"\"\"\n",
    "    Generates rewritten versions of the original user query\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates rewritten_queries key with a list of rewritten queries\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING REWRITTEN VERSIONS OF THE USER'S QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    num_rewrites = 3\n",
    "    rewritten_query_prompt_formatted = rewritten_query_prompt.format(question=question, num_rewrites=num_rewrites)\n",
    "    response = rewritten_query_llm.invoke(\n",
    "        [SystemMessage(content=rewritten_query_system_prompt)] + [HumanMessage(content=rewritten_query_prompt_formatted)]\n",
    "    )\n",
    "    rewritten_queries = response.rewritten_queries\n",
    "\n",
    "    return {\"rewritten_queries\": rewritten_queries}\n",
    "def continue_to_retrieval_nodes(state: GraphState):\n",
    "    edges_to_create = []\n",
    "    # Add original question\n",
    "    edges_to_create.append(Send(\"retrieve_documents\", {\"question\": state[\"question\"]}))\n",
    "    # Add rewritten queries\n",
    "    for rewritten_query in state[\"rewritten_queries\"]:\n",
    "        edges_to_create.append(Send(\"retrieve_documents\", {\"question\": rewritten_query}))\n",
    "    return edges_to_create\n",
    "\n",
    "# Define our graph\n",
    "graph_builder = StateGraph(GraphState, input=InputState, output=OutputState)\n",
    "graph_builder.add_node(\"generate_rewritten_queries\", generate_rewritten_queries)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "\n",
    "graph_builder.add_edge(START, \"generate_rewritten_queries\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_rewritten_queries\",\n",
    "    continue_to_retrieval_nodes,\n",
    "    [\"retrieve_documents\"]\n",
    ")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_response\",\n",
    "    grade_hallucinations,\n",
    "    {\n",
    "        \"supported\": END,\n",
    "        \"not supported\": \"generate_response\"\n",
    "    })\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value.\n",
    "\n",
    "Let's try both of these streaming modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "for chunk in graph.stream({\"question\": question}, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's just look at the keys for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "for chunk in graph.stream({\"question\": question}, stream_mode=\"updates\"):\n",
    "    print(chunk.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's stream the full state every time with `values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "for chunk in graph.stream({\"question\": question}, stream_mode=\"values\"):\n",
    "    print(chunk.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "async for event in graph.astream_events({\"question\": question}, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_to_stream = 'generate_response'\n",
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "async for event in graph.astream_events({\"question\": question}, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the response for this particular node stream back one token at at time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every example so far, [state has been transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution. If we invoke our graph for a second time, we are starting with a fresh state.\n",
    "\n",
    "This limits our ability to have multi-turn conversations with interruptions. \n",
    "\n",
    "We can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this! \n",
    " \n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step. This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting our State for multi-turn conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we set up memory in our application, let's edit our State and Nodes so that instead of acting a single \"question\", we instead act on a list of \"questions and answers\".\n",
    "\n",
    "We'll call our list \"messages\". These existing messages will all be used for our retrieval step. And at the end of our flow when our LLM responds, we will add the latest question and answer to our \"messages\" history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AnyMessage, get_buffer_string\n",
    "import operator\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]     # We now track a list of messages\n",
    "    generation: str\n",
    "    rewritten_queries: List[str]\n",
    "    documents: Annotated[List[Document], custom_documents_reducer]\n",
    "    attempted_generations: int\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]     # We output messages now in our OutputState\n",
    "    documents: List[Document]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's edit our existing Nodes to use `messages` instead of `question` when generating sample answers, grading document relevance, and generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_answer_system_prompt = \"\"\"You are a novice in charge of taking a conversation and a user's latest question as input, and generating a sample answer for the latest question.\\n\n",
    "This sample answer should contain words that would likely be in a real answer, but is not grounded in any factual documents, the way a real answer would be.\"\"\"\n",
    "sample_answer_prompt = \"This is the conversation so far {conversation} \\n\\n. Here is the user's latest question: \\n\\n {question}.\"\n",
    "def retrieve_documents(state):\n",
    "    print(\"---RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    messages = state.get(\"messages\", [])\n",
    "    conversation = get_buffer_string(messages)\n",
    "    print(conversation)\n",
    "\n",
    "    sample_answer_prompt_formatted = sample_answer_prompt.format(question=question, conversation=conversation)  # We use the entire chat history from messages to generate sample answers\n",
    "    response = sample_answer_llm.invoke(\n",
    "        [SystemMessage(content=sample_answer_system_prompt)] + [HumanMessage(content=sample_answer_prompt_formatted)]\n",
    "    )\n",
    "    sample_answer = response.sample_answer\n",
    "    documents = retriever.invoke(f\"{question}: {sample_answer}\")    # Now we use our question and sample answer\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_documents_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a conversation between a user and an AI assistant, and user's latest question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, definitely grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals that are not relevant at all. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_documents_prompt = \"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the conversation so far: \\n\\n {conversation} \\n\\n Here is the user question: \\n\\n {question}\"\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    conversation = get_buffer_string(state[\"messages\"])\n",
    "\n",
    "    # -- New logic to deduplicate documents our queries --\n",
    "    doc_counter = Counter(doc.page_content for doc in documents)\n",
    "    most_common_contents = doc_counter.most_common(5)\n",
    "    top_documents = []\n",
    "    for content, _ in most_common_contents:\n",
    "        for d in documents:\n",
    "            if d.page_content == content:\n",
    "                top_documents.append(d)\n",
    "                break\n",
    "\n",
    "    # Score each one of our five most common documents\n",
    "    filtered_docs = []\n",
    "    for d in top_documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question, conversation=conversation)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": {\"type\": \"overwrite\", \"documents\": filtered_docs}, \"question\": question, \"messages\": [HumanMessage(content=question)]}   # We add the question to Messages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(state: GraphState):\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    documents = state[\"documents\"]\n",
    "    conversation = get_buffer_string(state[\"messages\"])\n",
    "    attempted_generations = state.get(\"attempted_generations\", 0)   # By default we set attempted_generations to 0 if it doesn't exist yet\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    # RAG generation\n",
    "    rag_prompt_formatted = RAG_PROMPT_WITH_MESSAGES.format(context=formatted_docs, conversation=conversation)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"generation\": generation,\n",
    "        \"attempted_generations\": attempted_generations + 1   # In our state update, we increment attempted_generations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we'll add one more node in our graph which configures memory ahead of our next invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_memory(state):\n",
    "    generation = state[\"generation\"]\n",
    "    return {\n",
    "        \"messages\": [generation],   # Add generation to our messages_list\n",
    "        \"attempted_generations\": 0,   # Reset this value to 0\n",
    "        \"documents\": {\"type\": \"overwrite\", \"documents\": []}    # Reset documents to empty\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now let's define our graph and add some local memory!\n",
    "\n",
    "One of the easiest to work with is `MemorySaver`, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph\n",
    "graph_builder = StateGraph(GraphState, input=InputState, output=OutputState)\n",
    "graph_builder.add_node(\"generate_rewritten_queries\", generate_rewritten_queries)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "graph_builder.add_node(\"configure_memory\", configure_memory)    # New node for configuring memory\n",
    "\n",
    "graph_builder.add_edge(START, \"generate_rewritten_queries\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_rewritten_queries\",\n",
    "    continue_to_retrieval_nodes,\n",
    "    [\"retrieve_documents\"]\n",
    ")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_response\",\n",
    "    grade_hallucinations,\n",
    "    {\n",
    "        \"supported\": \"configure_memory\",\n",
    "        \"not supported\": \"generate_response\"\n",
    "    })\n",
    "graph_builder.add_edge(\"configure_memory\", END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use memory, we need to specify a `thread_id`.\n",
    "\n",
    "This `thread_id` will store our collection of graph states.\n",
    "\n",
    "* The checkpointer write the state at every step of the graph\n",
    "* These checkpoints are saved in a thread \n",
    "* We can access that thread in the future using the `thread_id`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "for chunk in graph.stream({\"question\": question}, config, stream_mode=\"values\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask a follow-up with the same thread_id!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "question = \"What about for building a code assistant?\"\n",
    "for chunk in graph.stream({\"question\": question}, config, stream_mode=\"values\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's take a look in LangSmith__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we compiled our graph with local memory - but realistically, you'll want to hook this up to a database. Here, we'll show how to use [Sqlite as a checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer), but other checkpointers, such as [Postgres](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/) are available!\n",
    "\n",
    "Create a file from your notebooks folder that sits in a directory /state_db/, called example.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "db_path = \"state_db/example.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "memory = SqliteSaver(conn)\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's invoke our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_2 = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id_2}}\n",
    "question = \"Can I use LangGraph for customer support? I want to create an agent application.\"\n",
    "graph.invoke({\"question\": question}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that our state is saved locally still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id_2}}\n",
    "graph_state = graph.get_state(config)\n",
    "graph_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using database like Sqlite means state is persisted! \n",
    "\n",
    "For example, we can re-start the notebook kernel and see that we can still load from Sqlite DB on disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakpoints and Human-in-the-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's talk about the motivations for `human-in-the-loop`:\n",
    "\n",
    "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
    "\n",
    "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
    "\n",
    "(3) `Editing` - You can modify the state \n",
    "\n",
    "LangGraph offers several ways to get or update agent state to support various `human-in-the-loop` workflows.\n",
    "\n",
    "First, we'll introduce [breakpoints](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/#simple-usage), which provide a simple way to stop the graph at specific steps. \n",
    "\n",
    "We'll show how this enables user `approval`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we are concerned about the quality of our retrieved documents, even after grading. All we need to do is simply compile the graph with `interrupt_after=[\"grade_documents\"]`.\n",
    "\n",
    "This means that the execution will be interrupted after the node `grade_documents` executes. Now, we can inspect the documents before generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=memory, interrupt_after=[\"grade_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_3 = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id_3}}\n",
    "question = \"Can I use LangGraph for building a code assistant?\"\n",
    "for chunk in graph.stream({\"question\": question}, config, stream_mode=\"values\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the state and look at the next node to call.\n",
    "\n",
    "This is a nice way to see that the graph has been interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll introduce a nice trick.\n",
    "\n",
    "When we invoke the graph with `None`, it will just continue from the last state checkpoint!\n",
    "\n",
    "![breakpoints.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbae7985b747dfed67775d_breakpoints1.png)\n",
    "\n",
    "For clarity, LangGraph will re-emit the current state.\n",
    "\n",
    "And then it will proceed to execute the following steps in the graph, which start with the `generate_response` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(None, config, stream_mode=\"values\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets bring these together with a specific user approval step that accepts user input.\n",
    "\n",
    "This is our human-in-the-loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_4 = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id_4}}\n",
    "question = \"Can I use LangGraph for building a customer support bot?\"\n",
    "for chunk in graph.stream({\"question\": question}, config, stream_mode=\"values\"):\n",
    "    print(chunk)\n",
    "\n",
    "# Get user feedback\n",
    "user_approval = input(\"Do these documents look good to you? (yes/no): \")\n",
    "\n",
    "# Check approval\n",
    "if user_approval.lower() == \"yes\":\n",
    "    \n",
    "    # If approved, continue the graph execution\n",
    "    for chunk in graph.stream(None, config, stream_mode=\"values\"):\n",
    "        print(chunk)\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also specify `interrupt_before` to pause execution before a certain node executes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's take a look in LangSmith__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakpoints are set by the developer on a specific node during graph compilation. \n",
    "\n",
    "But, sometimes it is helpful to allow the graph **dynamically interrupt** itself!\n",
    "\n",
    "This is an internal breakpoint, and [can be achieved using `NodeInterrupt`](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/dynamic_breakpoints/#run-the-graph-with-dynamic-interrupt).\n",
    "\n",
    "This has a few specific benefits: \n",
    "\n",
    "(1) you can do it conditionally (from inside a node based on developer-defined logic).\n",
    "\n",
    "(2) you can communicate to the user why its interrupted (by passing whatever you want to the `NodeInterrupt`).\n",
    "\n",
    "Let's add a `NodeInterrupt` to our `grade_documents` node instead of always interrupting after it! Specifically, let's only interrupt if no relevant documents can be found, instead of directly ending execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import NodeInterrupt\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    conversation = get_buffer_string(state[\"messages\"])\n",
    "\n",
    "    # -- New logic to deduplicate documents our queries --\n",
    "    doc_counter = Counter(doc.page_content for doc in documents)\n",
    "    most_common_contents = doc_counter.most_common(5)\n",
    "    top_documents = []\n",
    "    for content, _ in most_common_contents:\n",
    "        for d in documents:\n",
    "            if d.page_content == content:\n",
    "                top_documents.append(d)\n",
    "                break\n",
    "\n",
    "    # Score each one of our five most common documents\n",
    "    filtered_docs = []\n",
    "    for d in top_documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question, conversation=conversation)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    # Note: Added interrupt here!\n",
    "    if len(filtered_docs) == 0:\n",
    "        raise NodeInterrupt(f\"There are no relevant documents!: {filtered_docs}\")\n",
    "    \n",
    "    return {\"documents\": {\"type\": \"overwrite\", \"documents\": filtered_docs}, \"question\": question, \"messages\": [HumanMessage(content=question)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph\n",
    "graph_builder = StateGraph(GraphState, input=InputState, output=OutputState)\n",
    "graph_builder.add_node(\"generate_rewritten_queries\", generate_rewritten_queries)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "graph_builder.add_node(\"configure_memory\", configure_memory)\n",
    "\n",
    "graph_builder.add_edge(START, \"generate_rewritten_queries\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_rewritten_queries\",\n",
    "    continue_to_retrieval_nodes,\n",
    "    [\"retrieve_documents\"]\n",
    ")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_response\",\n",
    "    grade_hallucinations,\n",
    "    {\n",
    "        \"supported\": \"configure_memory\",\n",
    "        \"not supported\": \"generate_response\"\n",
    "    })\n",
    "graph_builder.add_edge(\"configure_memory\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_5 = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id_5}}\n",
    "question = \"Who is Man City's coach?\"\n",
    "graph.invoke({\"question\": question}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config)\n",
    "print(state.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're stuck here! As we are running the same node, we can't progress. To complete this toy example, we can edit the state here with `update_state`, and then continue. Let's add a document about Man City's coach.\n",
    "\n",
    "Note: These update use the same reducers that you defined in your State class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.update_state(\n",
    "    config,\n",
    "    {\"documents\": {\"type\": \"overwrite\", \"documents\": [Document(page_content=\"Man City's coach is Pep Guardiola!\")]}},\n",
    "    as_node=\"grade_documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(None, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awaiting User Input as a Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's clear that we can edit our agent state after a breakpoint.\n",
    "\n",
    "Now, what if we want to allow for human feedback to perform this state update?\n",
    "\n",
    "We'll add a node that [serves as a placeholder for human feedback](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/#setup) within our agent.\n",
    "\n",
    "This `human_feedback` node allow the user to add feedback directly to state.\n",
    " \n",
    "We specify the breakpoint using `interrupt_before` our `human_feedback` node.\n",
    "\n",
    "We set up a checkpointer to save the state of the graph up until this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no-op node that should be interrupted on\n",
    "def human_feedback(state: GraphState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of our NodeInterrupt in `grade_documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    conversation = get_buffer_string(state.get(\"messages\", []))\n",
    "\n",
    "    # -- New logic to deduplicate documents our queries --\n",
    "    doc_counter = Counter(doc.page_content for doc in documents)\n",
    "    most_common_contents = doc_counter.most_common(5)\n",
    "    top_documents = []\n",
    "    for content, _ in most_common_contents:\n",
    "        for d in documents:\n",
    "            if d.page_content == content:\n",
    "                top_documents.append(d)\n",
    "                break\n",
    "\n",
    "    # Score each one of our five most common documents\n",
    "    filtered_docs = []\n",
    "    for d in top_documents:\n",
    "        grade_documents_prompt_formatted = grade_documents_prompt.format(document=d.page_content, question=question, conversation=conversation)\n",
    "        score = grade_documents_llm.invoke(\n",
    "            [SystemMessage(content=grade_documents_system_prompt)] + [HumanMessage(content=grade_documents_prompt_formatted)]\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    return {\"documents\": {\"type\": \"overwrite\", \"documents\": filtered_docs}, \"question\": question, \"messages\": [HumanMessage(content=question)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph\n",
    "graph_builder = StateGraph(GraphState, input=InputState, output=OutputState)\n",
    "graph_builder.add_node(\"generate_rewritten_queries\", generate_rewritten_queries)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "graph_builder.add_node(\"configure_memory\", configure_memory)\n",
    "graph_builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "graph_builder.add_edge(START, \"generate_rewritten_queries\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_rewritten_queries\",\n",
    "    continue_to_retrieval_nodes,\n",
    "    [\"retrieve_documents\"]\n",
    ")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": \"human_feedback\"\n",
    "    })\n",
    "graph_builder.add_edge(\"human_feedback\", \"generate_response\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_response\",\n",
    "    grade_hallucinations,\n",
    "    {\n",
    "        \"supported\": \"configure_memory\",\n",
    "        \"not supported\": \"generate_response\"\n",
    "    })\n",
    "graph_builder.add_edge(\"configure_memory\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we stop execution on our Node, and continue once we receive human feedback!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_6 = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": thread_id_6}}\n",
    "question = \"Who is Man City's coach?\"\n",
    "graph.invoke({\"question\": question}, config)\n",
    "\n",
    "user_input = input(\"Add a document to state: \")\n",
    "\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\"documents\": {\"type\": \"overwrite\", \"documents\": [Document(page_content=user_input)]}},\n",
    "    as_node=\"grade_documents\"\n",
    ")\n",
    "graph.invoke(None, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's take a look in LangSmith__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's take a look in LangGraph Studio__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
